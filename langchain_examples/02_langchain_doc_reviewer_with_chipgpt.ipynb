{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rough Plan\n",
    "# Make sure backend is running before running this script localhost:8080 or ping the correct url\n",
    "# Function to do a check on backend\n",
    "\n",
    "# Use langgraph to create a doc reviewer\n",
    "\n",
    "# TODO: Use an agent to decided weather to retreive or not create basic agents to do non-retreival tasks like what time it is? dont do rag for that but use a tool\n",
    "\n",
    "# Once retrieved, use a LLM to do the review and use the following example to do the review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import Dict, Any, List, Optional\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Annotated, Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "import os\n",
    "import logging\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paste in the azure auth token here\n",
    "AUTH_TOKEN = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_context(query: str, tags: List[str], customer_uuid: str = None, filter_data: dict = None) -> List[str]:\n",
    "    \"\"\"\n",
    "    Fetches context from the backend.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query to send to the backend.\n",
    "        tags (List[str]): Tags to filter the context.\n",
    "        customer_uuid (str): Customer UUID for filtering.\n",
    "        filter_data (dict): Additional filter data.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: The list of relevant contexts retrieved from the backend.\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"query\": query,\n",
    "        \"tags\": tags,\n",
    "        \"customerUuid\": customer_uuid or \"\",\n",
    "        \"filterData\": filter_data or {}\n",
    "    }\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {AUTH_TOKEN}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"http://localhost:8080/api/context\", json=payload, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    context_list = response.json().get(\"context\", [])\n",
    "\n",
    "    if not isinstance(context_list, list):\n",
    "        raise ValueError(\"The 'context' key in the response must be a list.\")\n",
    "\n",
    "    return context_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_documents(contexts: List[str], question: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Grades each context string for relevance to the question.\n",
    "\n",
    "    Args:\n",
    "        contexts (List[str]): List of context strings to grade.\n",
    "        question (str): The user question.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of relevant contexts that passed the grading.\n",
    "    \"\"\"\n",
    "    relevant_contexts = []\n",
    "    for idx, context in enumerate(contexts):\n",
    "        logging.info(f\"Grading context {idx + 1}/{len(contexts)}: {context}\")\n",
    "\n",
    "        # Grader logic\n",
    "        model = AzureChatOpenAI(\n",
    "            temperature=0, \n",
    "            azure_endpoint=os.environ[\"OPENAI_API_HOST\"],\n",
    "            azure_deployment='gpt-4o-mini',\n",
    "            openai_api_version='2024-07-18', \n",
    "            streaming=True\n",
    "        )\n",
    "        \n",
    "        # model = AzureChatOpenAI(\n",
    "        #     temperature=0,\n",
    "        #     azure_endpoint=os.environ[\"OPENAI_API_HOST\"],\n",
    "        #     azure_deployment=\"gpt-4o\",\n",
    "        #     openai_api_version=\"2024-11-20\",\n",
    "        #     streaming=True,\n",
    "        # )\n",
    "        prompt = PromptTemplate(\n",
    "            template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \n",
    "            Here is the retrieved document: {context}\n",
    "            Here is the user question: {question}\n",
    "            If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant.\n",
    "            Give a binary score 'yes' or 'no'.\"\"\",\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "        )\n",
    "        chain = prompt | model\n",
    "        result = chain.invoke({\"context\": context, \"question\": question})\n",
    "\n",
    "        # Extract the content from the AIMessage object\n",
    "        if isinstance(result, AIMessage):\n",
    "            result_content = result.content.strip()\n",
    "        else:\n",
    "            logging.error(f\"Unexpected result type: {type(result)}\")\n",
    "            continue\n",
    "\n",
    "        if result_content.lower() == \"yes\":\n",
    "            relevant_contexts.append(context)\n",
    "        else:\n",
    "            logging.warning(f\"Context {idx + 1} dropped: {context}\")\n",
    "\n",
    "    # Log reordering if necessary\n",
    "    if relevant_contexts != contexts:\n",
    "        logging.info(\"The order of contexts has been modified after grading.\")\n",
    "\n",
    "    return relevant_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_langgraph_workflow():\n",
    "    \"\"\"\n",
    "    Creates a LangGraph workflow for document review using the backend API.\n",
    "\n",
    "    Returns:\n",
    "        StateGraph: The compiled LangGraph workflow.\n",
    "    \"\"\"\n",
    "    # Define the state schema\n",
    "    from pydantic import BaseModel\n",
    "\n",
    "    class AgentState(BaseModel):\n",
    "        messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "        contexts: Optional[List[str]] = None\n",
    "        graded_contexts: Optional[List[str]] = None  # Store indices of graded contexts\n",
    "        dropped_contexts: Optional[List[str]] = None  # Store indices of dropped contexts\n",
    "        graded_indices: Optional[List[int]] = None\n",
    "        dropped_indices: Optional[List[int]] = None\n",
    "        result: Optional[str] = None\n",
    "\n",
    "    def agent(state: AgentState):\n",
    "        \"\"\"\n",
    "        Fetches contexts from the backend and adds them to the state.\n",
    "        \"\"\"\n",
    "        messages = state.messages\n",
    "        question = messages[0].content\n",
    "        tags = [\"CHIPGPT\"]\n",
    "\n",
    "        logging.info(f\"Agent received question: {question}\")\n",
    "        contexts = fetch_context(query=question, tags=tags)\n",
    "\n",
    "        logging.info(f\"Fetched {len(contexts)} contexts from the backend.\")\n",
    "        state.contexts = contexts\n",
    "        return state\n",
    "\n",
    "    def grade_wrapper(state: AgentState):\n",
    "        \"\"\"\n",
    "        Grades the contexts and updates the state with graded contexts or a message indicating insufficient context.\n",
    "        \"\"\"\n",
    "        messages = state.messages\n",
    "        question = messages[0].content\n",
    "        contexts = state.contexts\n",
    "\n",
    "        logging.info(f\"Grading contexts for question: {question}\")\n",
    "\n",
    "        if not contexts:\n",
    "            logging.info(\"No contexts to grade.\")\n",
    "            state.result = \"no\"\n",
    "            return state\n",
    "\n",
    "        graded_contexts = grade_documents(contexts, question)\n",
    "\n",
    "        logging.info(f\"Graded contexts: {graded_contexts}\")\n",
    "        \n",
    "        # Determine dropped contexts by index\n",
    "        graded_indices = [i for i, context in enumerate(contexts) if context in graded_contexts]\n",
    "        dropped_indices = [i for i in range(len(contexts)) if i not in graded_indices]\n",
    "\n",
    "        state.graded_indices = graded_indices\n",
    "        state.dropped_indices = dropped_indices\n",
    "        # Determine dropped contexts\n",
    "        dropped_contexts = [context for context in contexts if context not in graded_contexts]\n",
    "        state.dropped_contexts = dropped_contexts\n",
    "\n",
    "        if not graded_contexts:\n",
    "            logging.info(\"No relevant contexts found.\")\n",
    "            state.messages = messages + [HumanMessage(content=\"I don't have enough relevant context to give you an answer.\")]\n",
    "            state.result = \"no\"\n",
    "        else:\n",
    "            state.graded_contexts = graded_contexts\n",
    "            state.result = \"yes\"\n",
    "\n",
    "        return state\n",
    "\n",
    "    def get_result(state: AgentState):\n",
    "        \"\"\"\n",
    "        Returns the result of the grading step.\n",
    "        \"\"\"\n",
    "        return state.result\n",
    "\n",
    "    def generate(state: AgentState):\n",
    "        \"\"\"\n",
    "        Generates the final response using the graded contexts.\n",
    "        \"\"\"\n",
    "        messages = state.messages\n",
    "        question = messages[0].content\n",
    "        docs = state.graded_contexts\n",
    "\n",
    "        logging.debug(f\"Docs received in generate: {docs} (type: {type(docs)})\")\n",
    "\n",
    "        if not docs:\n",
    "            logging.info(\"No contexts available for generation.\")\n",
    "            state.messages = messages + [HumanMessage(content=\"I don't have enough relevant context to give you an answer.\")]\n",
    "            return state\n",
    "\n",
    "        docs_combined = \"\\n\".join(docs)\n",
    "\n",
    "        logging.info(f\"Generating response for question: {question}\")\n",
    "        logging.info(f\"Using graded contexts: {docs_combined}\")\n",
    "\n",
    "        prompt = PromptTemplate(\n",
    "            template=\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \n",
    "                    Question: {question}  \n",
    "                    Context: {context}  \n",
    "                    Answer:\"\"\",\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "        )\n",
    "\n",
    "        llm = AzureChatOpenAI(\n",
    "            temperature=0,\n",
    "            azure_endpoint=os.environ[\"OPENAI_API_HOST\"],\n",
    "            azure_deployment=\"gpt-4o\",\n",
    "            openai_api_version=\"2024-11-20\",\n",
    "            streaming=True,\n",
    "        )\n",
    "        rag_chain = prompt | llm | StrOutputParser()\n",
    "        response = rag_chain.invoke({\"context\": docs_combined, \"question\": question})\n",
    "\n",
    "        logging.info(f\"Generated response: {response}\")\n",
    "        state.messages = messages + [response]\n",
    "        return state\n",
    "\n",
    "    def not_enough_context(state: AgentState):\n",
    "        \"\"\"\n",
    "        Handles cases where there's not enough context.\n",
    "        \"\"\"\n",
    "        return state  # State already has the message set in grade_wrapper\n",
    "\n",
    "    def log_summary(state: AgentState):\n",
    "        \"\"\"\n",
    "        Logs a summary of the workflow execution, including dropped context indices and the final order of contexts.\n",
    "        \"\"\"\n",
    "        logging.info(\"Workflow Summary:\")\n",
    "        logging.info(f\"Original context count: {len(state.contexts)}\")\n",
    "        logging.info(f\"Dropped context indices: {state.dropped_indices}\")\n",
    "        logging.info(f\"Final graded context indices (used in response): {state.graded_indices}\")\n",
    "        return state\n",
    "\n",
    "    # Use the defined schema for the state\n",
    "    workflow = StateGraph(AgentState)\n",
    "    workflow.add_node(\"agent\", agent)\n",
    "    workflow.add_node(\"grade_wrapper\", grade_wrapper)\n",
    "    workflow.add_node(\"generate\", generate)\n",
    "    workflow.add_node(\"not_enough_context\", not_enough_context)\n",
    "    workflow.add_node(\"log_summary\", log_summary)\n",
    "\n",
    "    # Add edges\n",
    "    workflow.add_edge(START, \"agent\")\n",
    "    workflow.add_edge(\"agent\", \"grade_wrapper\")\n",
    "\n",
    "    # Use get_result here\n",
    "    workflow.add_conditional_edges(\"grade_wrapper\", get_result, {\"yes\": \"generate\", \"no\": \"not_enough_context\"})\n",
    "\n",
    "    workflow.add_edge(\"generate\", \"log_summary\")\n",
    "    workflow.add_edge(\"not_enough_context\", \"log_summary\")\n",
    "    workflow.add_edge(\"log_summary\", END)\n",
    "\n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    # Create the LangGraph workflow  \n",
    "    graph = create_langgraph_workflow()  \n",
    "    \n",
    "    # Define the input messages using HumanMessage  \n",
    "    from langchain_core.messages import HumanMessage  \n",
    "    \n",
    "    inputs = {  \n",
    "        \"messages\": [  \n",
    "            HumanMessage(content=\"What is Nomic Embed?\")  # Use HumanMessage instead of dict  \n",
    "        ]  \n",
    "    }  \n",
    "    \n",
    "    # Stream the outputs from the graph  \n",
    "    try:  \n",
    "        for output in graph.stream(inputs):  \n",
    "            print(output)  # Output will include the final AIMessage\n",
    "    except Exception as e:  \n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc-academy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
