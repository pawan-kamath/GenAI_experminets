{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# State altering agents: stand up comedy\n",
    "\n",
    "In this example, we are going to create two agents. We'll provide them with `system prompts` to ask them to play the role of a comedian and we'll initiate a conversation between them. This time, this conversation will be a chat, instead of a simple `generate_reply()` request, meaning that each answer will alter the state of each agent. This example is a standard example that is used in the autogen documentation.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from autogen_agentchat.agents import Conn\n",
    "from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\n",
    "from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor\n",
    "from autogen_agentchat.teams import SelectorGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core.tools import FunctionTool\n",
    "\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Conversable Agent\n",
    "\n",
    "Let's important the ConversableAgent class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_client = AzureOpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_version=\"2024-11-20\",\n",
    "    azure_endpoint=os.getenv(\"OPENAI_API_HOST\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Let us now define our agents, we'll give them names, our chatGPT3.5 config and a `system prompt` to let them know that they are a stand-up comedian who is part of a two-person comedy show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bret = ConversableAgent(\n",
    "    name=\"Bret\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"Your name is Bret and you are a stand-up comedian in a two-person comedy show.\",\n",
    ")\n",
    "jemaine = ConversableAgent(\n",
    "    name=\"Jemaine\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"Your name is Jemain and you are a stand-up comedian in a two-person comedy show.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "And now that we have our agents, we can start a chat between them. This time, we'll use the `initiate_chat()` function from one of the agents instead of the `generate_reply()`. This function will require a receiver and an initiation message. We will also specify a number of turns after what the conversation will stop.  \n",
    "We will also store the result of this exchange in an object called `chat_result`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_result = bret.initiate_chat(\n",
    "    recipient = jemaine,\n",
    "    message=\"Jemaine, tell me a joke.\", \n",
    "    max_turns=2 # The conversation will stop after each agent has spoken twice\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "You should have been able to read an exchange between the two agents! You will notice that they might for example remember each other's name and other elements about each other, their state is affected by the conversation. This is your first conversation between LLM agents, congrats!  \n",
    "You can have some fun if you'd like by creating two different agents, give them different roles, historical ones, different setups and let them talk to practice.  \n",
    "\n",
    "## Better explore chat results\n",
    "\n",
    "During this exchange, the only element we received is the chat exchange. We might want to further explore it. We can do so using the following elements.\n",
    "\n",
    "We are going to import the `pprint` standard library from python to display somee elements of the chat exchange. We can start by displaying the chat history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(chat_result.chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "This gives us the whole exchange in a structured format that can be exported or re-used elsewhere in our program.\n",
    "\n",
    "We can also get a summary of how much this chat has cost us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(chat_result.cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "And finally we can also have a summary of the chat, but in this case, we are using the default mode which means that the summary is the last message... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(chat_result.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Since it would be better to have a summary that is a real summary and not the last message, we're going to re-run this exchange by specifying an additional argument to have a real summary of the exchange."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Chat summary\n",
    "\n",
    "We are going to re-run the initiation of the chat, without re-defining the agents, but this time will add two arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_result = bret.initiate_chat(\n",
    "    recipient = jemaine, \n",
    "    message=\"I'm Bret. Jemaine, let's keep the jokes rolling.\", \n",
    "    max_turns=2, \n",
    "    summary_method=\"reflection_with_llm\", # Can be \"last_message\" (DEFAULT) or \"reflection_with_llm\"\n",
    "    summary_prompt=\"Summarize the conversation\", # We specify the prompt used to summarize the chat\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "And this time, if we take a look at the summary, the result we'll get is the same as if the whole conversation was sent to chatGPT3.5 with the prompt \"Summarize the conversation\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(chat_result.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Termination\n",
    "\n",
    "Finally, the last element we'll explore about a two-person chat is how to end the conversation. Until now, we've used the argument `max_turns=2` to end the conversation after two turns. But we could also let the agents decide when they're done and finish the conversation then. \n",
    "\n",
    "To do that, we will have to tell each agent which words they should use when they're done and we'll have to monitor their messages for those words. Once autogen detects that the agent sent those words, the conversation will end. \n",
    "\n",
    "Since this is an agent setting, we'll have to re-define our agents this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "bret = ConversableAgent(\n",
    "    name=\"Bret\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"Your name is Bret and you are a stand-up comedian in a two-person comedy show. \"\n",
    "    \"When you're ready to end the conversation, say 'I gotta go'.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=lambda msg: \"I gotta go\" in msg[\"content\"],\n",
    ")\n",
    "jemaine = ConversableAgent(\n",
    "    name=\"Jemaine\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"Your name is Jemain and you are a stand-up comedian in a two-person comedy show. \"\n",
    "    \"When you're ready to end the conversation, say 'I gotta go'.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=lambda msg: \"I gotta go\" in msg[\"content\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Note how we told each agent to tell us when they're done with \"*When you're ready to end the conversation, say 'I gotta go'.*\" and how we added an `is_termination_msg` argument that looks into the sequence of characters `I gotta go` in each message using a python `lambda` function.\n",
    "\n",
    "Ok, let's now run this chat, and this time we will not specify a `max_turns=2`.  \n",
    "We will also not specify a LLM based summary method because we won't use it and running it will cost us some token, so we'll only specify it if we need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_result = bret.initiate_chat(\n",
    "    recipient = jemaine, \n",
    "    message=\"I'm Bret. Jemaine, let's keep the jokes rolling, let's start with jokes about therapists.\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "And the conversation should have ended exactly when one of the two comedians used the words `I gotta go`!\n",
    "\n",
    "Finally, the last element we'll explore is that we can substitute ourselves for an agent and interrogate the other agent about the conversation that just happened using a simple `send()` function. For example, let's ask Bret about the last joke Jemaine told him:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "jemaine.send(message=\"What's the last joke we talked about?\", recipient=bret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "And Bret is able to answer and tell us about the last joke Jemaine told him! We now have agents with states that evolve with the conversation. This is the simplest case, a two-agent conversation, with autogen, we can orchestrate much more complex setups to solve more advanced tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Other examples (homework)\n",
    "\n",
    "The example we just explored is a simple one that uses jokes, something everybody understands and is familiar with as a support to explain how agents can interact. But you could use agents to stage historical conversations or explore complex subjects by opposing two point of views supported by agents who'll remain calm and measured during the exchange.\n",
    "\n",
    "Here's an example you could try if you'd like:  \n",
    "\n",
    "> What if we staged a debate between Satoshi Nakamoto, the creator of Bitcoin and a proponent of hard sound money and Stephanie Kelton one of the main supporters of Modern Monetary Theory, an economic framwork that advocates for easy money to accomplish goals the government deem to be the best for everyone.\n",
    "\n",
    "Here's a code you coud run if you'd like to try such a scenario:\n",
    "\n",
    "```python\n",
    "# Define agent Stephanie\n",
    "stephanie = ConversableAgent(\n",
    "    name=\"Stephanie\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"Your name is Stephanie Kelton and you are a leading proponent of Modern Monetary Theory. \"\n",
    "    \"Modern Monetary Theory (MMT) is a heterodox economic theory which states that governments should not worry about government borrowing but be willing to aim for full employment, achieving it through expansionary fiscal policy and financing by creating money. \"\n",
    "    \"You are taking part in a debate about the future of money. \"\n",
    "    \"When you're ready to end the conversation, say 'Thank you for having me'.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=lambda msg: \"Thank you for having me\" in msg[\"content\"],\n",
    ")\n",
    "\n",
    "# Define agent Satoshi\n",
    "satoshi = ConversableAgent(\n",
    "    name=\"Satoshi\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"Your name is Satoshi Nakamoto and you are a the creator of Bitcoin. \"\n",
    "    \"The monetary policy of Bitcoin, characterized by a fixed supply capped at 21 million coins, contrasts with the Modern Monetary Theory (MMT) approach. \"\n",
    "    \"You are taking part in a debate about the future of money. \"\n",
    "    \"When you're ready to end the conversation, say 'Thank you for having me.'.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=lambda msg: \"Thank you for having me.\" in msg[\"content\"],\n",
    ")\n",
    "\n",
    "# Initiate the chat\n",
    "chat_result = stephanie.initiate_chat(\n",
    "    recipient = satoshi, \n",
    "    message=\"I'm Stephanie Kelton, and I'm happy to be taking part in this debate about the future of money and monetary policy. \"\n",
    "    \"I'd like to start by asking Satoshi why do they think that the government could not just provide a guaranteed job to all of its citizens?\", \n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent Stephanie\n",
    "stephanie = ConversableAgent(\n",
    "    name=\"Stephanie\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"Your name is Stephanie Kelton and you are a leading proponent of Modern Monetary Theory. \"\n",
    "    \"Modern Monetary Theory (MMT) is a heterodox economic theory which states that governments should not worry about government borrowing but be willing to aim for full employment, achieving it through expansionary fiscal policy and financing by creating money. \"\n",
    "    \"You are taking part in a debate about the future of money. \"\n",
    "    \"When you're ready to end the conversation, say 'Thank you for having me'.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=lambda msg: \"Thank you for having me\" in msg[\"content\"],\n",
    ")\n",
    "\n",
    "# Define agent Satoshi\n",
    "satoshi = ConversableAgent(\n",
    "    name=\"Satoshi\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"Your name is Satoshi Nakamoto and you are a the creator of Bitcoin. \"\n",
    "    \"The monetary policy of Bitcoin, characterized by a fixed supply capped at 21 million coins, contrasts with the Modern Monetary Theory (MMT) approach. \"\n",
    "    \"You are taking part in a debate about the future of money. \"\n",
    "    \"When you're ready to end the conversation, say 'Thank you for having me.'.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=lambda msg: \"Thank you for having me.\" in msg[\"content\"],\n",
    ")\n",
    "\n",
    "# Initiate the chat\n",
    "chat_result = stephanie.initiate_chat(\n",
    "    recipient = satoshi, \n",
    "    message=\"I'm Stephanie Kelton, and I'm happy to be taking part in this debate about the future of money and monetary policy. \"\n",
    "    \"I'd like to start by asking Satoshi why do they think that the government could not just provide a guaranteed job to all of its citizens?\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
